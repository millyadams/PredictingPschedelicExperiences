{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline - Training binary classifiers to predict mystical experiences \n",
    "#This script uses Absolute Power and LZ as input features \n",
    "\n",
    "#Import libraries and functions \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#Train/test \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Eval metrics \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report,  precision_recall_fscore_support, make_scorer \n",
    "#Models \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier \n",
    "#Feature selection \n",
    "from sklearn.feature_selection import RFECV\n",
    "#Feature importance\n",
    "import shap \n",
    "#save best model \n",
    "from joblib import dump\n",
    "import json\n",
    "import os\n",
    "#import homemade functions \n",
    "from project_functions import load_data, preprocess_data, feature_selection, plot_rfecv, evaluate_model, plot_cm, shap_analysis, confidence_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Load and process data \n",
    "#Load feature matrix \n",
    "data = load_data('/Users/millyadams/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Research Project/Data/PredictingExperiences_Paper/FollowUp_Paper/FeatureExtraction/Relative_and_Absolute/CSV/feature_df_mapped_ARL.csv') \n",
    "\n",
    "#Remove 'extra' features in the dataframe and separate into X and Y \n",
    "\n",
    "Entropy_columns = [col for col in data.columns if col.startswith('Entropy')] \n",
    "LZ_columns = [col for col in data.columns if col.startswith(('LZ', 'HilbertLZ'))] \n",
    "relpower = [col for col in data.columns if col.startswith('Rel')]\n",
    "absopower = [col for col in data.columns if col.startswith('Abso')]\n",
    "\n",
    "X = data.drop(Entropy_columns + relpower+ ['SubjectID', 'MEQ_Class', 'VRS_Class', 'Anx_Class'], axis = 1) #X contains input features \n",
    "y = data['MEQ_Class'] #y contains classification labels \n",
    "print(\"Number of features:\", len(X.columns)) #validate \n",
    "#Process data - split and scale \n",
    "X_train, X_test, y_train, y_test = preprocess_data(X, y, test_size=0.2, random_state=42) #preprocessing steps defined in a function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data shapes before proceed \n",
    "validate_shape = {\n",
    "    'Category ': [\n",
    "        'Features (x)', 'Labels (y)', 'Train Features (X_train)', 'Test Features (X_test)'\n",
    "        ],\n",
    "    'Shape': [ \n",
    "        X.shape, y.shape, X_train.shape, X_test.shape\n",
    "        ]\n",
    "}\n",
    "validate_shape_df = pd.DataFrame(validate_shape)\n",
    "class_distribution = {\n",
    "    'Class Distribution': [\n",
    "        'Overall', 'Train', 'Test'\n",
    "    ],\n",
    "    'Count':\n",
    "        [y.value_counts().to_dict(), y_train.value_counts().to_dict(), y_test.value_counts().to_dict()]\n",
    "}\n",
    "class_distribution_df = pd.DataFrame(class_distribution)\n",
    "\n",
    "print('Data Shapes')\n",
    "print(validate_shape_df)\n",
    "print('Class Distribtions')\n",
    "print(class_distribution_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment for imbalanced data \n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "## Apply SMOTE to the training data\n",
    "#smote = SMOTE(random_state=42) #sampling_strategy=0.75)\n",
    "#X_train, y_train = smote.fit_resample(X_train, y_train) #apply smote to training datasets \n",
    "\n",
    "#print('\\nTrain Class Distribution:')\n",
    "#print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Train default models to establish baseline performance \n",
    "rs = 42 #set random state \n",
    "\n",
    "#Define models (dictionary)- linear and non linear models considering small dataset, models chosen as also SHAP compatable \n",
    "models = {\n",
    "    \"SVM\": SVC(kernel= 'linear', probability=True, random_state=rs), #consistent random state \n",
    "    \"Random_Forest\": RandomForestClassifier(random_state=rs),\n",
    "    \"Logistic_Regression\": LogisticRegression(random_state=rs),\n",
    "    \"Decision_Tree\": DecisionTreeClassifier(random_state=rs),\n",
    "    \"Gradient_Boosting\": GradientBoostingClassifier(random_state=rs),\n",
    "    \"Ada_Boosting\": AdaBoostClassifier(algorithm='SAMME', random_state=rs),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=rs)\n",
    "    \n",
    "}\n",
    "\n",
    "#set zero division - metric set to zero when the score is undefined - e.g. no positive predictions made by model, does not impact model but how metric is reported\n",
    "precision = make_scorer(precision_score, zero_division=0)\n",
    "recall = make_scorer(recall_score, zero_division=0)\n",
    "f1 = make_scorer(f1_score, zero_division=0)\n",
    "\n",
    "#set scoring metrics - dictionary used as variables are defined above \n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)  \n",
    "\n",
    "#create dictionary to store results \n",
    "results = {\n",
    "    'Model': [],\n",
    "    'Accuracy Mean': [],\n",
    "    'Accuracy Std': [],\n",
    "    'Accuracy CI Lower':[],\n",
    "    'Accuracy CI Upper': [],\n",
    "    'F1 Mean': [],\n",
    "    'Precision Mean': [],\n",
    "    'Recall Mean': []\n",
    "}\n",
    "\n",
    "#Test default models \n",
    "for name, model in models.items(): \n",
    "    cv_results = cross_validate(model, X_train, y_train, cv=cv, scoring=scoring) \n",
    "    accuracy_scores = cv_results['test_accuracy']\n",
    "    accuracy_ci_lower, accuracy_ci_upper = confidence_interval(accuracy_scores)\n",
    "\n",
    "    #append the data for each model to the lists within the dictionary \n",
    "    results['Model'].append(name)\n",
    "    results['Accuracy Mean'].append(cv_results['test_accuracy'].mean())\n",
    "    results['Accuracy Std'].append(cv_results['test_accuracy'].std())\n",
    "    results['Accuracy CI Lower'].append(accuracy_ci_lower)\n",
    "    results['Accuracy CI Upper'].append(accuracy_ci_upper)\n",
    "    results['F1 Mean'].append(cv_results['test_f1'].mean())\n",
    "    results['Precision Mean'].append(cv_results['test_precision'].mean())\n",
    "    results['Recall Mean'].append(cv_results['test_recall'].mean()) \n",
    "\n",
    "results_data = pd.DataFrame(results) \n",
    "\n",
    "#Save results \n",
    "\n",
    "file_path = '/Users/millyadams/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Research Project/Data/PredictingExperiences_Paper/FollowUp_Paper/RESULTS/MYSTICAL/AbsoLZ/default_models.csv'\n",
    "results_data.to_csv(file_path, index = False)\n",
    "results_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Feature selection for dimensionality reduction \n",
    "#RFECV automatically finds the optimal number of features by iterating over different numbers and selecting the one that yields the best cross-validation score\n",
    "#Best choice when unsure about the optimal number of features and want the method to determine it empirically\n",
    "\n",
    "feature_names = X.columns\n",
    "\n",
    "#Initialise dictionaries to store features, selected features and trained models \n",
    "selected_features = {}\n",
    "rfe_data = {} \n",
    "trained_models = {}\n",
    "results_rfe = [] #store results of cv evaluation \n",
    "\n",
    "\n",
    "#Loop to conduct RFECV, save optimal feature set and plot on graph \n",
    "for name, model in models.items(): \n",
    "    print(f\"Model: {name}\") \n",
    "    \n",
    "    # Perform feature selection using RFECV for models other than RF and DT\n",
    "    if name in [\"Random_Forest\", \"Decision_Tree\"]:\n",
    "        # Use all features for RF and DT\n",
    "        X_train_SF = X_train.copy()  # Keep all features\n",
    "        X_test_SF = X_test.copy()  # Keep all features\n",
    "        support = None  # No feature selection applied\n",
    "        optimal_features = X_train_SF.shape[1]  # All features are optimal\n",
    "                #store reduced feature set for model in dictionary \n",
    "        rfe_data[name] = {\n",
    "            'X_train_SF': X_train_SF,\n",
    "            'X_test_SF' : X_test_SF\n",
    "        }\n",
    "        selected_features[name] = feature_names\n",
    "\n",
    "    else:\n",
    "        \n",
    "        X_train_SF, X_test_SF, support, rfecv, optimal_features = feature_selection(X_train, X_test, y_train, model, cv) #predefined function for feature selection \n",
    "        selected_features[name] = feature_names[rfecv.support_].tolist() #filters names of selected features and converts to list to store in selected features dictionary with model name as the key \n",
    "        \n",
    "        #store reduced feature set for model in dictionary \n",
    "        rfe_data[name] = {\n",
    "            'X_train_SF': X_train_SF,\n",
    "            'X_test_SF' : X_test_SF\n",
    "        }\n",
    "\n",
    "        #print optimal number of features \n",
    "        print(f\"Optimal number of features for {name}: {rfecv.n_features_}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        #train the model and evaluate the performance of the model on reduced feature set \n",
    "        model.fit(X_train_SF, y_train)\n",
    "        trained_models[name] = model #store trained model in dictionary \n",
    "        \n",
    "        #extract performance metrics on cross validation \n",
    "        cv_results_rfe = cross_validate(model, X_train_SF, y_train, cv=cv, scoring=scoring) #eval trained model \n",
    "        \n",
    "        #report selected features and metrics \n",
    "        #create dictionary to store results \n",
    "        result = {\n",
    "            'Model' : name, \n",
    "            'Optimal number of features' : optimal_features, \n",
    "            'Selected features': selected_features[name]\n",
    "        }\n",
    "        \n",
    "        #loop through performance metrics from cv_results_rfe and calculate mean for each metric\n",
    "        for metric in scoring:\n",
    "            mean_score = cv_results_rfe[f'test_{metric}'].mean() #cv_results_rfe is a dictionary returned by cross_validate function containing the results of cross validation \n",
    "            result[f'CV Mean {metric}'] = mean_score #store mean cv score for metric in dictionary \n",
    "        \n",
    "        results_rfe.append(result)\n",
    "\n",
    "results_rfe_df = pd.DataFrame(results_rfe)\n",
    "file_path = '/Users/millyadams/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Research Project/Data/PredictingExperiences_Paper/FollowUp_Paper/RESULTS/MYSTICAL/AbsoLZ/rfecv_results.csv'\n",
    "#save results\n",
    "results_rfe_df.to_csv(file_path, index = False)\n",
    "results_rfe_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Paramerer optimisation - hyperparameter tuning with random search \n",
    "#Define random search grid for each model \n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_range = {\n",
    "\"SVM\": {\n",
    "        'kernel': ['rbf', 'linear', 'poly'],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1, 1, 10],\n",
    "        'C': sp_uniform(0.01, 100),  # Use uniform for continuous values\n",
    "        'degree': sp_randint(1, 10),  # Applicable for 'poly' kernel\n",
    "        'coef0': sp_uniform(0.0, 1.0)  # Use uniform for continuous values\n",
    "    }, \n",
    "\"Random_Forest\": {\n",
    "        'n_estimators': sp_randint(10, 200),\n",
    "        'max_depth': [None] + list(range(10, 31)),\n",
    "        'min_samples_split': sp_randint(2, 10),\n",
    "        'min_samples_leaf': sp_randint(1, 5),\n",
    "        'bootstrap': [True, False], \n",
    "        \"criterion\": [\"gini\", \"entropy\"]\n",
    "    }, \n",
    "\"Logistic_Regression\": {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # Include all penalties - elas not chosen, remove?\n",
    "    'C': sp_uniform(0.01, 100),  # Regularization strength\n",
    "    'solver': ['liblinear', 'saga'],  # Choose compatible solvers\n",
    "    'l1_ratio': sp_uniform(0.0, 1.0),  # Only needed for elasticnet\n",
    "    'max_iter': [100, 200, 300],  # Increase max_iter if does not converge \n",
    "    'tol': [1e-4, 1e-3, 1e-2],  # Tolerance for stopping criteria\n",
    "    'class_weight': [None, 'balanced'],  # Handle class imbalance\n",
    "},\n",
    "\"Decision_Tree\": {\n",
    "        'criterion': ['gini', 'entropy'],  \n",
    "        'max_depth': [None] + list(range(1, 21)),  \n",
    "        #'min_samples_split': sp_randint(2, 11),  \n",
    "        #'min_samples_leaf': sp_randint(1, 11), \n",
    "        'class_weight': [None, 'balanced'],  \n",
    "    },\n",
    "    \n",
    "\"Gradient_Boosting\": {\n",
    "        'learning_rate': np.linspace(0.01, 0.3, num=20),  # Adjusted upper limit\n",
    "        'n_estimators': sp_randint(10, 200),  \n",
    "        'max_depth': sp_randint(3, 10),  \n",
    "        #'min_samples_split': sp_randint(2, 10), \n",
    "        #'min_samples_leaf': sp_randint(1, 10),  \n",
    "    },\n",
    " \"Ada_Boosting\": {\n",
    "        'estimator': [None, RandomForestClassifier(), DecisionTreeClassifier(), GradientBoostingClassifier()],\n",
    "        'n_estimators': sp_randint(50, 200),\n",
    "        'learning_rate': np.linspace(0.01, 2, num=20)\n",
    "    },\n",
    " \"XGBoost\": { \n",
    "        'booster': ['gbtree', 'gblinear'],\n",
    "        'learning_rate': np.linspace(0.01, 0.3, num=20), \n",
    "        'eval_metric': ['rmse', 'mae', 'logloss', 'error'],\n",
    "        'n_estimators': sp_randint(50, 300),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise dictionaries to store best models and scores\n",
    "best_models = {}\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "permutation_scores_dict = {}\n",
    "\n",
    "results = {\n",
    "    'Model': [],\n",
    "    'Best Hyperparameters': [],\n",
    "    'Best Cross-validation Score': [],\n",
    "    'Mean Test Accuracy': [],\n",
    "    'SD':[],\n",
    "    'Mean Test F1': [],\n",
    "    'Mean Test Precision': [],\n",
    "    'Mean Test Recall': [],\n",
    "}\n",
    "\n",
    "n_iter_dict = {\n",
    "    \"SVM\": 40, \n",
    "    \"Random_Forest\": 40, \n",
    "    \"Logistic_Regression\": 40, \n",
    "    \"Decision_Tree\": 40, \n",
    "    \"Gradient_Boosting\": 40, \n",
    "    \"Ada_Boosting\": 40, \n",
    "    \"XGBoost\": 40\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    current_param_range = param_range[name] #extract parameter grid for model from predefined dictionary \n",
    "    X_train = rfe_data[name]['X_train_SF']\n",
    "    \n",
    "    rand_search = RandomizedSearchCV(\n",
    "        estimator = model, \n",
    "        param_distributions=current_param_range, \n",
    "        n_iter=n_iter_dict[name], \n",
    "        cv=cv, \n",
    "        scoring = scoring, \n",
    "        refit = 'f1',\n",
    "        return_train_score=True) \n",
    "    rand_search.fit(X_train, y_train) #fit model with grid search and X_train containing optimal number of features \n",
    "    \n",
    "    best_models[name] = rand_search.best_estimator_ #store best model \n",
    "    best_params[name] = rand_search.best_params_ #store best hyperparamters for model \n",
    "    best_scores[name] = rand_search.best_score_ #store best cv score for model \n",
    "    \n",
    "    print(f\"Model: {name}\") #debug \n",
    "    \n",
    "    cv_results = rand_search.cv_results_\n",
    "    mean_test_scores = {metric: cv_results[f'mean_test_{metric}'][rand_search.best_index_] for metric in scoring}\n",
    "    \n",
    "    std_test_accuracy = cv_results['std_test_accuracy'][rand_search.best_index_]  # Get standard deviation of accuracy scores\n",
    "    \n",
    "    results['Model'].append(name)\n",
    "    results['Best Hyperparameters'].append(best_params[name])\n",
    "    results['Best Cross-validation Score'].append(best_scores[name])\n",
    "    results['Mean Test Accuracy'].append(mean_test_scores['accuracy'])\n",
    "    results['SD'].append(std_test_accuracy)\n",
    "    results['Mean Test F1'].append(mean_test_scores['f1'])\n",
    "    results['Mean Test Precision'].append(mean_test_scores['precision'])\n",
    "    results['Mean Test Recall'].append(mean_test_scores['recall'])\n",
    "\n",
    "results_hpt_df = pd.DataFrame(results)\n",
    "file_path = '/Volumes/Harddrive/Paper_followup/Results/Mystical_Experiences/rand_search_results.csv'\n",
    "#save results \n",
    "results_hpt_df.to_csv(file_path, index=False)\n",
    "results_hpt_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Test on unseen data \n",
    "#create dictionary for test results to convert into df \n",
    "test_results = {\n",
    "   'Model' : [],\n",
    "   'Test_accuracy':[],\n",
    "   'Test_f1': [],\n",
    "   'Test_precision':[],\n",
    "   'Test_recall':[],\n",
    "   'Confusion_matrix':[],\n",
    "\n",
    "}\n",
    "\n",
    "class_names = ['Non-Mystical', 'Mystical']\n",
    "savedir = '/Users/millyadams/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Research Project/Data/PredictingExperiences_Paper/FollowUp_Paper/RESULTS/MYSTICAL/AbsoLZ/CMs/'\n",
    "\n",
    "for name, model in best_models.items():\n",
    "   #Retrieve test set with selected features for specific model\n",
    "    X_test = rfe_data[name]['X_test_SF']\n",
    "    print(f'Model: {name}')\n",
    "    accuracy, f1, precision, recall, cm = evaluate_model(model, X_test, y_test)\n",
    "    plot_cm(cm, class_names = class_names, name = name, savedir = savedir)\n",
    "    \n",
    "    #add to test data dictionary \n",
    "    test_results['Model'].append(name)\n",
    "    test_results['Test_accuracy'].append(accuracy)\n",
    "\n",
    "    test_results['Test_f1'].append(f1)\n",
    "    test_results['Test_precision'].append(precision)\n",
    "    test_results['Test_recall'].append(recall)\n",
    "    test_results['Confusion_matrix'].append(cm)\n",
    "    \n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "file_path = '/Users/millyadams/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Research Project/Data/PredictingExperiences_Paper/FollowUp_Paper/RESULTS/MYSTICAL/AbsoLZ/test_results.csv'\n",
    "#save \n",
    "test_results_df.to_csv(file_path, index=False)\n",
    "test_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Feature importance - SHAP analysis and visualisation \n",
    "savedir = '/Users/millyadams/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Research Project/Data/PredictingExperiences_Paper/FollowUp_Paper/RESULTS/MYSTICAL/AbsoLZ/SHAP/'\n",
    "\n",
    "sns.set(style=\"white\") #change to white grid if want gird \n",
    "\n",
    "#Perform SHAP analysis on all models \n",
    "for name, model in best_models.items(): #for all models in best_models dictionary (saved in HPT) \n",
    "\n",
    "    X_train = rfe_data[name]['X_train_SF'] #extract X datasets with only selected features for that model \n",
    "    X_test = rfe_data[name]['X_test_SF']\n",
    "    \n",
    "    selected_features_for_model = selected_features[name] #identify selected features for model \n",
    "    \n",
    "    # Debug print statements to verify data\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Selected features: {selected_features_for_model}\")\n",
    "    print(f\"X_train_rfe shape: {X_train.shape}\")\n",
    "    print(f\"X_test_rfe shape: {X_test.shape}\")\n",
    "    \n",
    "    # Ensure X_train_rfe and X_test_rfe are DataFrames - columns for selected features \n",
    "    X_train = pd.DataFrame(X_train, columns=selected_features_for_model)\n",
    "    X_test = pd.DataFrame(X_test, columns=selected_features_for_model)\n",
    "    \n",
    "    shap_analysis(model, X_train, X_test, name, selected_features_for_model, savedir=savedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
